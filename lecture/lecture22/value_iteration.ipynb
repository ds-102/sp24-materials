{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proprietary-emphasis",
   "metadata": {},
   "source": [
    "Definitions:\n",
    "* The Q-function tells us the sum of total (discounted) rewards if we start in state s, take action a, and then act optimally from there\n",
    "* The value function tells us the sum of total (discounted) rewards if we start in state s and act optimally from there\n",
    "* When there's no ambiguity, we'll drop the$\\,^*$ and use $V(s)$ and $Q(s, a)$ instead of the more precise $V^*(s)$ and $Q^*(s, a)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(s, a) &= \\sum_{s'} T(s, a, s') \\big[ R(s, a, s') + \\gamma V(s') \\big] \\\\\n",
    "V(s) \n",
    "    &= \\max_a Q(s, a) \\\\\n",
    "    &= \\max_a \\sum_{s'} T(s, a, s') \\big[ R(s, a, s') + \\gamma V(s') \\big] \\\\\n",
    "\\pi(s) &= \\mathrm{arg}\\max_a Q(s, a)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined as part of the setup\n",
    "states = [...]  # all states\n",
    "actions = [...]  # all actions\n",
    "gamma = 0.9  # Discount factor\n",
    "T = 1000000  # max time\n",
    "\n",
    "def T(s, a, s_new):\n",
    "    \"\"\"\n",
    "    Probability of ending in state s_new when starting in state s and\n",
    "    taking action a\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "def R(s, a, s_new):\n",
    "    \"\"\"\n",
    "    Reward for going from state s to state s_new by taking action a\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "## SLOW VERSION\n",
    "def V(s, t):\n",
    "    \"\"\"\n",
    "    Value function for a state: expected reward for starting in\n",
    "    s at time t and then acting optimally\n",
    "    \"\"\"\n",
    "    if t < T:\n",
    "        # Take max over actions\n",
    "        best_so_far = -np.inf\n",
    "        for a in actions:\n",
    "            # Compute the expectation over possible next states\n",
    "            q = 0\n",
    "            for s_new in states:\n",
    "                transition_prob = T(s, a, s_new)\n",
    "                total_reward = R(s, a, s_new) + gamma * V(s_new, t+1)\n",
    "                q + = transition_prob * total_reward\n",
    "\n",
    "            value = max(q, best_so_far)\n",
    "            return value\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "## FAST VERSION\n",
    "V_arr = np.zeros([len(states), max_time])\n",
    "for t in range(max_time-1, -1, -1):\n",
    "    for s in states:\n",
    "        # Take max over actions\n",
    "        best_q = -np.inf\n",
    "        for a in actions:\n",
    "            # Compute the expectation over possible next states\n",
    "            q = 0  # Q(s, a)\n",
    "            for s_new in states:\n",
    "                q += T(s, a, s_new) * (R(s, a, s_new) + gamma * V_arr[s_new, t+1])\n",
    "\n",
    "            if q > best_q:\n",
    "                best_q = q\n",
    "\n",
    "        V_arr[s, t] = best_so_far\n",
    "\n",
    "def V_fast(state, time=0):\n",
    "    return V_arr[state, time]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b5412",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(s, a) &= \\sum_{s'} T(s, a, s') \\big[ R(s, a, s') + \\gamma V(s') \\big] \\\\\n",
    "V(s) \n",
    "    &= \\max_a Q(s, a) \\\\\n",
    "    &= \\max_a \\sum_{s'} T(s, a, s') \\big[ R(s, a, s') + \\gamma V(s') \\big]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bcdda4-607f-4f51-9c1a-32636ae4ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Q-iteration\" (not a commonly used term): a modification of value iteration\n",
    "# to save Q-values so that we can find policies\n",
    "\n",
    "\n",
    "## FAST VERSION\n",
    "Q_arr = np.zeros([len(states), len(actions), max_time])\n",
    "for t in range(max_time-1, -1, -1):\n",
    "    for s in states:\n",
    "        # Take max over actions\n",
    "        best_q = -np.inf\n",
    "        for a in actions:\n",
    "            # Compute the expectation over possible next states\n",
    "            q = 0  # Q(s, a)\n",
    "            for s_new in states:\n",
    "                # V(s_new, t+1) = max_a Q(s_new, a, t+1)\n",
    "                value = np.max(Q_arr[s_new, :, t+1])\n",
    "                q += T(s, a, s_new) * (R(s, a, s_new) + gamma * value)\n",
    "\n",
    "            Q_arr[s, a, t] = q\n",
    "\n",
    "        V_arr[s, t] = best_so_far\n",
    "\n",
    "def Q_fast(state, action, time=0):\n",
    "    return Q_arr[state, action, time]\n",
    "\n",
    "def policy(state, time=0):\n",
    "    action_idx = np.argmax(Q_arr[state, :, time])\n",
    "    return actions[action_idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
