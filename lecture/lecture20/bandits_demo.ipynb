{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d006b-454b-48f3-a884-58b4e70701b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d02386-94a5-4e56-8ca2-7f09666370c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-Armed Bandit\n",
    "#We have K = 9 coins with (unknown) success probabilities:\n",
    "pvals = np.arange(0.1, 1, 0.1)\n",
    "K = len(pvals)\n",
    "print(K)\n",
    "T = 1000 #Number of rounds that we will play for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac69e30-e4c7-46ad-b917-656dc628a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In each round, we pick a coin:\n",
    "i_pick = 3\n",
    "i_reward = np.random.binomial(1, pvals[i_pick+1])\n",
    "print(i_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b61363-58d1-4b30-ae7d-83eac6386afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random choice of coin at each round:\n",
    "cumu_reward = 0\n",
    "for tme in range(T):\n",
    "    i_pick = np.random.choice(len(pvals))\n",
    "    i_reward = np.random.binomial(1, pvals[i_pick])\n",
    "    cumu_reward = cumu_reward + i_reward\n",
    "print(cumu_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701532e-0456-4589-bada-a0225e4179fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "50*np.log(1/delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6672f3dc-d0f0-4e98-9027-9e8bd1e3a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore then Commit Algorithm:\n",
    "m = 50 #we will explore each coin for m rounds (overall there will be m*K rounds of exploration)\n",
    "allrounds_ETC = np.zeros((K, T))\n",
    "allrewards_ETC = np.zeros((K, T))\n",
    "cumu_reward_ETC = 0\n",
    "#Exploration phase:\n",
    "for tme in range(1, m*K + 1):\n",
    "    i_pick = (tme-1) % K\n",
    "    allrounds_ETC[i_pick, tme-1] = 1\n",
    "    i_reward = np.random.binomial(1, pvals[i_pick])\n",
    "    allrewards_ETC[i_pick, tme -1] = i_reward\n",
    "    cumu_reward_ETC = cumu_reward_ETC + i_reward\n",
    "print(cumu_reward_ETC)\n",
    "\n",
    "coin_sums = allrounds_ETC.sum(axis=1)\n",
    "print(coin_sums)\n",
    "\n",
    "cumu_reward_alternative = allrewards_ETC.sum()\n",
    "print(cumu_reward_alternative)\n",
    "\n",
    "num_tosses = allrounds_ETC.sum(axis=1)\n",
    "reward_coins = allrewards_ETC.sum(axis=1)\n",
    "phat = reward_coins/num_tosses\n",
    "print(phat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db413dd8-9457-4df6-b63d-eda35d01ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UCB Algorithm\n",
    "#First explore as in  ETC\n",
    "for tme in range(m*K+1, T+1):\n",
    "    #delta = 1/(tme ** 3)\n",
    "    delta = 1\n",
    "    num_tosses = allrounds_ETC.sum(axis=1)\n",
    "    reward_coins = allrewards_ETC.sum(axis=1)\n",
    "    phat = reward_coins/num_tosses\n",
    "    ucb = phat + np.sqrt((np.log(1/delta))/(2*num_tosses))\n",
    "    i_pick = np.argmax(ucb)\n",
    "    allrounds_ETC[i_pick, tme-1] = 1\n",
    "    i_reward = np.random.binomial(1, pvals[i_pick])\n",
    "    allrewards_ETC[i_pick, tme -1] = i_reward\n",
    "total_reward = allrewards_ETC.sum()\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1356a34-414e-4522-8e37-04afec32b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full UCB Algorithm:\n",
    "m = 1 #we will explore each coin for m rounds (overall there will be m*K rounds of exploration)\n",
    "allrounds_UCB = np.zeros((K, T))\n",
    "allrewards_UCB = np.zeros((K, T))\n",
    "cumu_reward_UCB = 0\n",
    "#Exploration phase:\n",
    "for tme in range(1, m*K + 1):\n",
    "    i_pick = (tme-1) % K\n",
    "    allrounds_UCB[i_pick, tme-1] = 1\n",
    "    i_reward = np.random.binomial(1, pvals[i_pick])\n",
    "    allrewards_UCB[i_pick, tme -1] = i_reward\n",
    "    cumu_reward_UCB = cumu_reward_UCB + i_reward\n",
    "for tme in range(m*K+1, T+1):\n",
    "    #delta = 1\n",
    "    delta = 1/(tme ** 3)\n",
    "    num_tosses = allrounds_UCB.sum(axis=1)\n",
    "    reward_coins = allrewards_UCB.sum(axis=1)\n",
    "    phat = reward_coins/num_tosses\n",
    "    ucb = phat + np.sqrt((np.log(1/delta))/(2*num_tosses))\n",
    "    i_pick = np.argmax(ucb)\n",
    "    allrounds_UCB[i_pick, tme-1] = 1\n",
    "    i_reward = np.random.binomial(1, pvals[i_pick])\n",
    "    allrewards_UCB[i_pick, tme -1] = i_reward\n",
    "total_reward = allrewards_UCB.sum()\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029feff5-660a-43ed-a0a1-46ff92a3ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many times was each coin picked?\n",
    "print(allrounds_UCB.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381300f-f3ec-48cb-b2cb-97dc53fe6e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cumulative Reward\n",
    "cumu_reward_UCB = np.cumsum(allrewards_UCB.sum(axis = 0))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cumu_reward_UCB)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Explore then Commit Algorithm')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16367cf-bc87-408b-bc25-9293c06828dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thompson Sampling\n",
    "allrounds_TS = np.zeros((K, T))\n",
    "allrewards_TS = np.zeros((K, T))\n",
    "for tme in range(1, T+1):\n",
    "    num_tosses = allrounds_TS.sum(axis=1)\n",
    "    reward_coins = allrewards_TS.sum(axis=1)\n",
    "    samples = [np.random.beta(reward_coins[i] + 1, num_tosses[i] - reward_coins[i]+1) for i in range(K)]\n",
    "    i_pick = np.argmax(samples)\n",
    "    allrounds_TS[i_pick, tme-1] = 1\n",
    "    i_reward = np.random.binomial(1, pvals[i_pick])\n",
    "    allrewards_TS[i_pick, tme -1] = i_reward\n",
    "total_reward_TS = allrewards_TS.sum()\n",
    "print(total_reward_TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53209575-d1f9-429f-bb50-dd37bdc93fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many times was each coin picked?\n",
    "print(allrounds_TS.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f6854-e424-438b-a987-158583eff9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting cumulative Reward\n",
    "cumu_reward_TS = np.cumsum(allrewards_TS.sum(axis = 0))\n",
    "\n",
    "plt.plot(cumu_reward_UCB, color = 'blue', label = 'UCB')\n",
    "plt.plot(cumu_reward_TS, color = 'red', label = 'TS')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Rewards of UCB and TS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b6617-f6d4-4b64-aa95-4d7101c0ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Regrets (instead of Rewards):\n",
    "cumu_regret_UCB = np.arange(1,T+1)*np.max(pvals) - cumu_reward_UCB\n",
    "cumu_regret_TS = np.arange(1,T+1)*np.max(pvals) - cumu_reward_TS\n",
    "\n",
    "plt.plot(cumu_regret_UCB, color = 'blue', label = 'UCB')\n",
    "plt.plot(cumu_regret_TS, color = 'red', label = 'TS')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Cumulative Regret')\n",
    "plt.title('Cumulative Regrets of UCB and TS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34afa829-69e7-4245-80b8-c8ea13f6d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Averaging over multiple simulation runs:\n",
    "#To get smooth regrets, we can average individual regrets over multiple simulation runs:\n",
    "\n",
    "#UCB:\n",
    "m = 1\n",
    "n_sims = 80\n",
    "allsims_UCB = np.zeros((n_sims, T))\n",
    "for sim in range(n_sims):\n",
    "    allrounds_UCB = np.zeros((K, T))\n",
    "    allrewards_UCB = np.zeros((K, T))\n",
    "    #Exploration phase:\n",
    "    for tme in range(m*K):\n",
    "        i_pick = tme % K\n",
    "        allrounds_UCB[i_pick, tme] = 1\n",
    "        i_reward = np.random.binomial(1, pvals[i_pick])\n",
    "        allrewards_UCB[i_pick, tme] = i_reward\n",
    "    for tme in range(m*K, T):\n",
    "        num_tosses = allrounds_UCB.sum(axis=1)\n",
    "        reward_coins = allrewards_UCB.sum(axis=1)\n",
    "        phat = reward_coins/num_tosses\n",
    "        delta = 1/(tme ** 3)\n",
    "        #delta = 1\n",
    "        ucb = phat + np.sqrt((np.log(1/delta))/(2*num_tosses))\n",
    "        i_pick = np.argmax(ucb)\n",
    "        allrounds_UCB[i_pick, tme] = 1\n",
    "        i_reward = np.random.binomial(1, pvals[i_pick])\n",
    "        allrewards_UCB[i_pick, tme] = i_reward\n",
    "    cumu_reward_UCB = np.cumsum(allrewards_UCB.sum(axis = 0))\n",
    "    cumu_regret_UCB = np.arange(1,T+1)*np.max(pvals) - cumu_reward_UCB\n",
    "    allsims_UCB[sim,:] = cumu_regret_UCB\n",
    "average_cumu_regret_UCB = np.mean(allsims_UCB, axis = 0)\n",
    "\n",
    "#For Thompson Sampling:\n",
    "allsims_TS = np.zeros((n_sims, T))\n",
    "for sim in range(n_sims):\n",
    "    allrounds_TS = np.zeros((K, T)) \n",
    "    allrewards_TS = np.zeros((K, T))\n",
    "    for tme in range(T):\n",
    "        num_tosses = allrounds_TS.sum(axis=1)\n",
    "        reward_coins = allrewards_TS.sum(axis=1)\n",
    "        samples = [np.random.beta(reward_coins[i] + 1, num_tosses[i] - reward_coins[i]+1) for i in range(K)]\n",
    "        i_pick = np.argmax(samples)\n",
    "        allrounds_TS[i_pick, tme] = 1\n",
    "        i_reward = np.random.binomial(1, pvals[i_pick])\n",
    "        allrewards_TS[i_pick, tme] = i_reward\n",
    "    cumu_reward_TS = np.cumsum(allrewards_TS.sum(axis = 0))\n",
    "    cumu_regret_TS = np.arange(1,T+1)*np.max(pvals) - cumu_reward_TS\n",
    "    allsims_TS[sim,:] = cumu_regret_TS\n",
    "average_cumu_regret_TS = np.mean(allsims_TS, axis = 0)\n",
    "\n",
    "plt.plot(average_cumu_regret_UCB, color = 'blue', label = 'UCB')\n",
    "plt.plot(average_cumu_regret_TS, color = 'red', label = 'TS')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Average Regret')\n",
    "plt.title('Averaged Cumulative Regrets of UCB and TS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
